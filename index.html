<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Maks Sorokin</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- <link rel="icon" type="image/png" href="assets/"> -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1J19VDQYCK"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-1J19VDQYCK');
  </script>
  <!-- Video playback speed and no controls -->
  <script defer src="js/video.js"></script>
</head>

<body>
  <div class="container">
    <!-- NAVBAR --------------------------------- -->
    <!-- <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href="index.html">About</a></li>
          <li class="navbar-item"><a class="navbar-link" href="/blog.html">Blog</a></li>
        </ul>
      </div>
    </nav> -->

    <!-- Personal Info  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row profile-row" style="margin-top: 4%">
      <div class="one columns"></div>
      <div class="three columns">
        <!-- Profile photo  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div style="text-align: center;">
          <img src="assets/img/photo.jpg" class="profile-photo" alt="profile photo"><br>
        </div>
        <!-- Name
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <h5 style="text-align: center;">Maks Sorokin</h5>
        <!-- Social links
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div id="social">
          <a href="https://twitter.com/initmaks"><img src="assets/icons/twitter.png" class="iico" /></a>
          <a rel="me" href="https://sigmoid.social/@maks"><img src="assets/icons/mastodon.png" class="iico" /></a>
          <a href="https://github.com/initmaks"><img src="assets/icons/github.png" class="iico" /></a>
          <a href="assets/pdfs/CV.pdf"><img src="assets/icons/file.png" class="iico" /></a>
          <img src="assets/icons/mail.png" style="cursor: pointer;" class="iico" id="iemail" title="click to reveal" />
        </div>
        <div id="demail"></div> <!-- will reveal email -->
      </div>

      <div class="eight columns" style="margin-top: 5%;">
        <p style="text-align:justify;">
          <!-- Self-introduction
          –––––––––––––––––––––––––––––––––––––––––––––––––– -->
          I'm a third-year Robotics Ph.D. student at Georgia Tech advised by
          <a href="https://www.cc.gatech.edu/~sha9/" style="text-decoration: none;">Dr. Sehoon Ha</a>
          and
          <a href="https://ckllab.stanford.edu/" style="text-decoration: none;">Dr. C. Karen Liu</a>.
          I am interested in applications of vision-based robot learning in real-world robotics.
          Currently, I am working on outdoor navigation and environment interaction problems.
        </p>
        <!-- <strong>Competences:</strong>
        <a>python</a>,
        <a>pytorch</a>,
        <a>pybullet</a>,
        <a>iGibson</a>,
        <a>OpenCV</a>,
        <a>numpy</a>,
        <a>Tensorflow</a>,
        <a>C/C++</a>,
        <a>ROS</a>,
        <a>docker</a> -->

        <h5><u>News</u></h5>
        <ul>
          <li> <b>APR'22</b> - Human Motion Control of Quadrupedal Robots accepted at RSS <a
              href="https://sites.google.com/view/humanconquad/">[project page]</a></li>
          <li> <b>SEP'21</b> - Excited to be joining <a href="https://x.company">X - the moonshot factory</a> ( <a
              href="https://everydayrobots.com">Everyday Robots</a> ) for PhD
            Residency!</li>
          <li> <b>SEP'21</b> - Check out our latest work on Learning Sidewalk Navigation <a
              href="./navigation.html">[project page]</a></li>
          <!-- <li> <b>MAY'21</b> - Awarded the fellowship by the Machine Learning Center at Georgia Tech. <a href="https://mlatgt.blog/2021/05/10/the-machine-learning-center-awards-inaugural-mlgt-fellows/">[link]</a> </li> -->
          <!-- <li> <b>FEB'21</b> Paper on Learning Human Search Behavior accepted at EUROGRAPHICS'2021! <a href="https://arxiv.org/pdf/2011.03618.pdf">[pdf]</a><a href="https://arxiv.org/abs/2011.03618">[arXiv]</a></li> -->
          <!-- <li> <b>DEC'20</b> Paper on Few-shot visual sensor meta-adaptation accepted at ICRA'2021! <a href="https://arxiv.org/pdf/2011.03609.pdf">[pdf]</a><a href="http://arxiv.org/abs/2011.03609">[arXiv]</a></li> -->
        </ul>


      </div>

    </div>
  </div>

  <!-- Latest
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <br>
    <div class="div_line"></div><br>
    <h5><u>Latest Work</u></h5>


    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <img src="assets/img/nav/small.png" style="width:300px;">
      </div>
      <div class="eight columns">
        <!-- TITLE
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div id="project_title">Learning to Navigate Sidewalks in Outdoor Environments</div>
        <!-- AUTHORS + VENUE
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <br><em><strong>Maks Sorokin</strong>, Jie Tan, C. Karen Liu, Sehoon Ha</em>
        <br><em>IEEE Robotics and Automation Letters (RA-L) 2022</em>
        <!-- DESCRIPTION
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p id="project_info">
          We design a system which enables zero-shot vision-based policy transfer to the real-world outdoor environments for sidewalk navigation task.
          Our approach is evaluated on a quadrupedal robot navigating sidewalks in the real world walking 3.2 kilometers with a limited number of human interventions.
        </p>

        <!-- LINKS
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="./navigation.html">[webpage]</a>
        <a href="https://www.youtube.com/watch?v=JsAZy3YETwQ">[video]</a>
        <a href="https://techxplore.com/news/2021-09-robot-efficiently-sidewalks-urban-environments.html">[TechXplore article]</a>
        
      </div>
    </div>

  </div>

  <!-- Publications
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <br>
    <div class="div_line"></div><br>
    <h5><u>Publications</u></h5>

    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video id="humanconquad" class="no-controls-video" autoplay loop muted preload="auto">
            <source src="assets/videos/humanconquad.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div id="project_title">Human Motion Control of Quadrupedal Robots using Deep Reinforcement Learning</div>
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <br><em>Sunwoo Kim, <strong>Maks Sorokin</strong>, Jehee Lee, Sehoon Ha</em>
        <br><em><span class="venue">Proceedings of Robotics: Science and Systems (RSS) 2022</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p id="project_info">
          We propose a novel motion control system that allows a human user to operate various motor tasks seamlessly
          on a quadrupedal robot.
          Using our system, a user can execute a variety of motor tasks, including standing, sitting, tilting,
          manipulating, walking, and turning, on simulated and real quadrupeds.
        </p>

        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://sites.google.com/view/humanconquad">[project page]</a>
        <a href="https://arxiv.org/pdf/2204.13336.pdf">[pdf]</a>
        <a href="https://arxiv.org/abs/2204.13336">[arXiv]</a>
        <a href="https://www.youtube.com/watch?v=kz8hBG1CKMY">[video]</a>
      </div>
    </div>

    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video class="no-controls-video" autoplay loop muted preload="auto">
            <source src="assets/videos/behavior_representations.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div id="project_title">Learning Behavior Representations Through Multi-Timescale Bootstrapping</div>
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <br><em>Mehdi Azabou, Michael Mendelson, <strong>Maks Sorokin</strong>, Shantanu Thakoor, Nauman Ahad, Carolina
          Urzay, Eva L Dyer</em>
        <br><em><span class="venue">CVPR Workshop on Multi-Agent Behavior (Oral), 2022</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p id="project_info">
          We introduce Bootstrap Across Multiple Scales (BAMS), a multi-scale representation learning model for
          behavior. We combine a pooling module that aggregates features extracted over encoders with different temporal
          receptive fields, and design latent objectives to bootstrap the representations in each respective
          space to encourage disentanglement across different timescales.
        </p>

        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://arxiv.org/pdf/2206.07041.pdf">[pdf]</a>
        <a href="https://arxiv.org/abs/2206.07041">[arXiv]</a>
        <a href="https://www.mehai.dev/assets/bams_poster.pdf">[poster]</a>
      </div>
    </div>

    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video class="no-controls-video" autoplay loop muted preload="auto">
            <source src="assets/videos/urban_navigation.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div id="project_title">Learning to Navigate Sidewalks in Outdoor Environments</div>
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <br><em><strong>Maks Sorokin</strong>, Jie Tan, C. Karen Liu, Sehoon Ha</em>
        <br><em><span class="venue">IEEE Robotics and Automation Letters (RA-L) 2022</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p id="project_info">
          We design a system which enables zero-shot vision-based policy transfer to the real-world outdoor environments
          for sidewalk navigation task.
          Our approach is evaluated on a quadrupedal robot navigating sidewalks in the real world walking 3.2 kilometers
          with a limited number of human interventions.
        </p>

        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="./navigation.html">[project page]</a>
        <a href="https://arxiv.org/pdf/2109.05603.pdf">[pdf]</a>
        <a href="https://arxiv.org/abs/2109.05603">[arXiv]</a>
        <a href="https://www.youtube.com/watch?v=JsAZy3YETwQ">[video]</a>
        <a href="https://techxplore.com/news/2021-09-robot-efficiently-sidewalks-urban-environments.html">[TechXplore
          article]</a>
      </div>
    </div>

    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video class="no-controls-video" autoplay loop muted preload="auto">
            <source src="assets/videos/object_search_animation.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div id="project_title">Learning Human Search Behavior from Egocentric View</div>
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <br><em><strong>Maks Sorokin</strong>, Wenhao Yu, Sehoon Ha, C. Karen Liu </em>
        <br><em><span class="venue">EUROGRAPHICS 2021</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p id="project_info">
          We train vision-based agent to perform object searching in photorealistic 3D scene.
          And propose a motion synthesis mechanism for head motion re-targeting.
          Using which we enable object searching behaviour with animated human character (PFNN/NSM).
        </p>

        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://arxiv.org/pdf/2011.03618.pdf">[pdf]</a>
        <a href="https://arxiv.org/abs/2011.03618">[arXiv]</a>
        <a href="https://www.youtube.com/watch?v=LvSHpmjt8pU">[video]</a>
        <a href="https://www.youtube.com/watch?v=NzsCT3a7rpY">[talk(20 min)]</a>
      </div>
    </div>


    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video class="no-controls-video" autoplay loop muted preload="auto">
            <source src="assets/videos/sensor_height_adaptation.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div id="project_title">A Few Shot Adaptation of Visual Navigation Skills to New Observations using
          Meta-Learning</div>
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <br><em>Qian Luo, <strong>Maks Sorokin</strong>, Sehoon Ha</em>
        <br><em><span class="venue">The IEEE International Conference on Robotics and Automation (ICRA) 2021</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p id="project_info">
          We show how vision-based navigation agents can be trained to adapt to new sensor configurations with only
          three shots of experience.
          Rapid adaptation is achieved by introducing a bottleneck between perception and control networks, and through
          the perception component's meta-adaptation.
        </p>
        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://arxiv.org/pdf/2011.03609.pdf">[pdf]</a>
        <a href="http://arxiv.org/abs/2011.03609">[arXiv]</a>
      </div>
    </div>

  </div>

  <!-- Projects
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <br>
    <div class="div_line"></div><br>
    <h5><u>Projects</u></h5>

    <!-- PROJECT ENTRY ###
    –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row" id="project_entry">
      <!-- MEDIA SECTION
      –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <img src="assets/img/sim2sim.png">
        </div>
      </div>
      <div class="eight columns">
        <!-- TITLE
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div id="project_title">Real2Sim Image adaptation</div>
        <br><em><span class="venue">2019</span></em>
        <!-- DESCRIPTION
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p id="project_info">
          Image domain adaptation through the conversion of images with randomized textures (or real images) to a
          canonical image representation.
          Replication of a <a href="https://arxiv.org/abs/1812.07252">RCAN paper</a> with different loss modeling (<a
            href="https://arxiv.org/abs/1603.08155">Perceptual/Feature Loss</a> instead of GAN loss).
        </p>
        <!-- LINKS
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://github.com/initmaks/ran2can">[github]</a>

      </div>
    </div>

    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video id="learning_to_swing" class="no-controls-video" autoplay loop muted preload="auto">
            <source src="assets/videos/learning_to_swing.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div id="project_title">Learning to swing</div>
        <br><em><span class="venue">2018</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p id="project_info">
          Computer Animation class project, which utilizes off-the-shelf Soft-Actor-Critic Reinforcement Learning method
          that learns to build up the momentum and swing the animated character on a pull up bar.
        </p>
        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="pages/charanim.html">[short-summary]</a>
      </div>
    </div>

    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video id="fetchit" class="no-controls-video" autoplay loop muted preload="auto">
            <source src="assets/videos/fetchit.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div id="project_title">FetchIt</div>
        <br><em><span class="venue">2018</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p id="project_info">
          Mobile Manipulation project that utilises MoveIt! & GQ-CNN to grasp an object from the table using a Fetch
          Robot in the Gazebo Simulator.
        </p>

        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="pages/fetchit.html">[short-summary]</a>
      </div>
    </div>


    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video id="car_bc" class="no-controls-video" autoplay loop muted preload="auto">
            <source src="assets/videos/drive.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div id="project_title">Behavioral Clonning for Autonomous Driving</div>
        <br><em><span class="venue">2017</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p id="project_info">
          End-to-end (image-to-steering wheel) control policy learning from data collected over multiple laps with
          off-the-track recoveries generated by human.
        </p>

        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://github.com/initmaks/Self-driving_car_ND/tree/master/Behavioral-Cloning">[github]</a>
      </div>
    </div>

  </div>

  <br>
  <br>

  <!-- Teaching –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="div_line"></div><br><br>
    <div class="row profile-row">
      <div class="one columns hide-on-small">&nbsp;</div>
      <div class="nine columns ">
        <h5><u>Mentoring Experience</u></h5>
        I've had a great pleasure working with a number of exceptional students at Georgia Tech.<br><br>
        <ul>
          <li><strong>NOW</strong>: Master's Student - <a href="https://jxu443.github.io/portfolio/">Jiaxi Xu</a></li>
          <li><strong>FALL 2021</strong>: Master's Student - Arjun Krishna</li>
          <li><strong>FALL 2020</strong>: Master's Student - <a href="https://qianluo.netlify.app">Qian Luo</a> -
            Currently an NLP Algorithm Engineer at Alibaba Group</li>
        </ul>
        <br>
        <h5><u>Teaching Experience</u></h5>
        I had an amazing experience helping teach one of the largest classes at Georgia Tech.
        <br>CS6601 - Artificial Intelligence class by
        <a href="https://www.cc.gatech.edu/people/thomas-ploetz"> Dr. Thomas Ploetz</a> & <a
          href="https://www.cc.gatech.edu/home/thad/">Dr. Thad Starner</a>.
        <br><br>
        <ul>
          <li><strong>FALL 2019 - SPRING 2020</strong>: Head Teaching Assistant </li>
          <li><strong>FALL 2018 - SPRING 2019</strong>: Teaching Assistant </li>
        </ul>
      </div>
    </div>
  </div>


  <!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- attempt hiding from spambots :p - snippet credits Andrej Karpathy - karpathy.ai source code -->
  <script type="text/javascript">
    var e_is_shown = false;
    document.getElementById('iemail').addEventListener("click", function () {
      let demail = document.getElementById('demail');
      demail.innerHTML = 'm' + 'aks' + ' _a' + 't_ ' + 'gatech' + '.' + 'e' + 'du';
      demail.style.opacity = e_is_shown ? 0 : 1;
      e_is_shown = !e_is_shown;
    })
  </script>


  <!-- ### FOOTER ### -->

  <br>
  <br>

  <br>
  <br>
  <div class="div_line"></div><br>

  <p style="color:gray; text-align:center; font-size: small;">
    <h7 style="text-decoration: none;">consider checking out: </h7><br><br>
    <a href="https://www.givewell.org/"><img src="assets/icons/gw.jpg" style="width:100px;"></a>
    <a href="https://www.givingwhatwecan.org/"><img src="assets/icons/gwwc.png" style="width:100px;"></a>
    <a href="https://www.effectivealtruism.org/"><img src="assets/icons/ea.png" style="width:80px; margin:10px"></a>

  </p>


  <div class="div_line"></div><br>

  <p style="color:gray; text-align:center; font-size: small;">
    2023© Maks Sorokin<br>
    built using <a href="http://getskeleton.com/">Skeleton</a>,
    icon credits <a href="https://www.flaticon.com/"> flaticon</a>,
    hosted by <a href="https://pages.github.com/"> GitHub Pages</a>❤️
    <br>
    <br>
    feel free to copy: <a href="https://github.com/initmaks/initmaks.github.io">this page</a>
  </p>

</body>

</html>